# -*- coding: utf-8 -*-
"""Simple_Scikit_Integration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/wandb/examples/blob/master/colabs/scikit/Simple_Scikit_Integration.ipynb

<a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/scikit/Simple_Scikit_Integration.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
<!--- @wandbcode{simple-sklearn} -->

<img src="https://wandb.me/logo-im-png" width="400" alt="Weights & Biases" />

<!--- @wandbcode{simple-sklearn} -->

# üèãÔ∏è‚Äç‚ôÄÔ∏è W&B + üß™ Scikit-learn
Use Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.


<img src="https://wandb.me/mini-diagram" width="650" alt="Weights & Biases" />


## What this notebook covers:
* Easy integration of Weights and Biases with Scikit.
* W&B Scikit plots for model interpretation and diagnostics for regression, classification, and clustering.

**Note**: Sections starting with _Step_ are all you need to integrate W&B to existing code.


## The interactive W&B Dashboard will look like this:

![](https://i.imgur.com/F1ZgR4A.png)
"""

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn import datasets, cluster

from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight

from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

"""## Step 0: Install W&B"""

#!pip install wandb -qU

"""## Step 1: Import W&B and Login"""

import wandb

wandb.login()

"""# Regression

**Let's check out a quick example**
"""

# Load data
housing = datasets.fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target
X, y = X[::2], y[::2]  # subsample for faster demo
wandb.errors.term._show_warnings = False
# ignore warnings about charts being built from subset of data

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Train model, get predictions
reg = Ridge()
reg.fit(X_train, y_train)
y_pred = reg.predict(X_test)

"""## Step 2: Initialize W&B run"""

run = wandb.init(project='my-scikit-integration', name="regression")

"""## Step 3: Visualize model performance

### Residual Plot

Measures and plots the predicted target values (y-axis) vs the difference between actual and predicted target values (x-axis), as well as the distribution of the residual error.

Generally, the residuals of a well-fit model should be randomly distributed because good models will account for most phenomena in a data set, except for random error.

[Check out the official documentation here $\rightarrow$](https://docs.wandb.com/library/integrations/scikit#residuals-plot)
"""

wandb.sklearn.plot_residuals(reg, X_train, y_train)

"""### Outlier Candidate

Measures a datapoint's influence on regression model via Cook's distance. Instances with heavily skewed influences could potentially be outliers. Useful for outlier detection.

[Check out the official documentation here $\rightarrow$](https://docs.wandb.com/library/integrations/scikit#outlier-candidates-plot)
"""

wandb.sklearn.plot_outlier_candidates(reg, X_train, y_train)

"""## All-in-one: Regression plot

Using this all in one API one can:
* Log summary of metrics
* Log learning curve
* Log outlier candidates
* Log residual plot
"""

wandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, model_name='Ridge')

wandb.finish()

